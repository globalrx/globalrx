{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0315b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% imports and setup\n",
    "\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "from Levenshtein import distance as levdistance\n",
    "# warning: possible code licensing issue with this package?\n",
    "# if that's a problem can replace Levenshtein.distance with nltk.edit_distance\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d5f2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found records for 1988 products\n",
      "Human         1706\n",
      "Veterinary     282\n",
      "Name: Category, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1706it [00:24, 69.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 0 files with 108 errors, 1598 already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1598/1598 [00:37<00:00, 43.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 0 PDFs to TXT, (0/1598) complete, 1598 errors\n",
      "('data/ema/pdf/EMEA_H_C_000913.pdf', b\"pdftotext version 4.04 [www.xpdfreader.com]\\nCopyright 1996-2022 Glyph & Cog, LLC\\nUsage: pdftotext [options] <PDF-file> [<text-file>]\\n  -f <int>               : first page to convert\\n  -l <int>               : last page to convert\\n  -layout                : maintain original physical layout\\n  -simple                : simple one-column page layout\\n  -simple2               : simple one-column page layout, version 2\\n  -table                 : similar to -layout, but optimized for tables\\n  -lineprinter           : use strict fixed-pitch/height layout\\n  -raw                   : keep strings in content stream order\\n  -fixed <number>        : assume fixed-pitch (or tabular) text\\n  -linespacing <number>  : fixed line spacing for LinePrinter mode\\n  -clip                  : separate clipped text\\n  -nodiag                : discard diagonal text\\n  -enc <string>          : output text encoding name\\n  -eol <string>          : output end-of-line convention (unix, dos, or mac)\\n  -nopgbrk               : don't insert a page break at the end of each page\\n  -bom                   : insert a Unicode BOM at the start of the text file\\n  -marginl <number>      : left page margin\\n  -marginr <number>      : right page margin\\n  -margint <number>      : top page margin\\n  -marginb <number>      : bottom page margin\\n  -opw <string>          : owner password (for encrypted files)\\n  -upw <string>          : user password (for encrypted files)\\n  -verbose               : print per-page status information\\n  -q                     : don't print any messages or errors\\n  -cfg <string>          : configuration file to use in place of .xpdfrc\\n  -listencodings         : list all available output text encodings\\n  -v                     : print copyright and version info\\n  -h                     : print usage information\\n  -help                  : print usage information\\n  --help                 : print usage information\\n  -?                     : print usage information\\n\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encountered problems reading 0 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data/ema/'\n",
    "pdf_dir = data_dir + 'pdf/'\n",
    "txt_dir = data_dir + 'txt/'\n",
    "\n",
    "RUN_DIAGNOSTICS = False\n",
    "\n",
    "#%% get master spreadsheet from EMA listing all product pages\n",
    "\n",
    "url = 'https://www.ema.europa.eu/sites/default/files/Medicines_output_european_public_assessment_reports.xlsx'\n",
    "filename = url.split('/')[-1]\n",
    "with open(filename, 'wb') as f:\n",
    "    f.write(requests.get(url).content)\n",
    "    \n",
    "\n",
    "#%% reload from disk (no need to download every time)\n",
    "\n",
    "data = pd.read_excel('Medicines_output_european_public_assessment_reports.xlsx', skiprows=8, header=0)\n",
    "print(f'Found records for {data.URL.nunique()} products')\n",
    "print(data.Category.value_counts())\n",
    "\n",
    "# we don't need the veterinary products\n",
    "data = data[data['Category']=='Human']\n",
    "\n",
    "#%% find all linked english pdfs with \"product-information\" in the filename,\n",
    "#   save each one with the product number (e.g. as the local filename\n",
    "\n",
    "new_files, existing_files, errors = 0, 0, []\n",
    "\n",
    "for index,row in tqdm(data.iterrows()):\n",
    "    product_number = re.sub('/', '_', row['Product number']).strip()\n",
    "    filename = pdf_dir + product_number + '.pdf'\n",
    "    if os.path.exists(filename):\n",
    "        existing_files += 1\n",
    "        continue\n",
    "    \n",
    "    url = row['URL']\n",
    "    soup = BeautifulSoup(requests.get(url).text, features='html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "    pdfs = [l['href'] for l in links if l['href'].endswith('_en.pdf')]\n",
    "    pdfs = [p for p in pdfs if '/product-information/' in p]\n",
    "    \n",
    "    if len(pdfs)==1: # there should be one product info document\n",
    "        link = pdfs[0]\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(requests.get(link).content)\n",
    "            new_files += 1            \n",
    "    else:\n",
    "        errors.append(index)\n",
    "\n",
    "print(f'Wrote {new_files} files with {len(errors)} errors, {existing_files} already downloaded')\n",
    "\n",
    "\n",
    "#%% optional: this code verifies that the drugs with no product information are withdrawn/refused\n",
    "\n",
    "if RUN_DIAGNOSTICS:\n",
    "    products = [re.sub('/', '_', p).strip() for p in data['Product number']]\n",
    "    files = os.listdir(pdf_dir)\n",
    "    missing = [p for p in products if p+'.pdf' not in files]\n",
    "    print(f'Missing product information sheet from {len(missing)}/{len(data)} pages')\n",
    "    \n",
    "    # Missing files are all for drugs that have been refused/withdrawn, normally that box says \"Authorized\"\n",
    "    ema_status = []\n",
    "    for p in missing:\n",
    "        row = data[data['Product number'] == re.sub('_','/',p)]\n",
    "        url = row['URL'].values[0]\n",
    "        soup = BeautifulSoup(requests.get(url).text, features='html.parser')\n",
    "        status = soup.find('div',{'class':'ema-status-title'})\n",
    "        text = 'Status not listed' if not status else status.text\n",
    "        ema_status.append(text)\n",
    "        \n",
    "    summary = pd.DataFrame({'file':missing, 'status':ema_status})\n",
    "    summary['code'] = data.loc[errors]['Product number'].values\n",
    "    summary['link'] = data.loc[errors]['URL'].values\n",
    "    print(summary.status.value_counts())\n",
    "    \n",
    "\n",
    "#%% functions to...\n",
    "#   strip tables and margins, pdfplumber is very clunky about this :/\n",
    "#   find headers within text of single file\n",
    "#   slice files with multiple entries into multiple outputs (e.g. check for repeated instances of section 1)\n",
    "#   write output to disk\n",
    "\n",
    "# helper function for pdfplumber\n",
    "def remove_tables(page):\n",
    "    ts = {\"vertical_strategy\": \"lines\", \"horizontal_strategy\": \"lines\"}\n",
    "    bboxes = [table.bbox for table in page.find_tables(table_settings=ts)]\n",
    "    \n",
    "    def not_within_bboxes(obj):\n",
    "        #Check if the object is in any of the table's bbox.\n",
    "        def obj_in_bbox(_bbox):\n",
    "            #See https://github.com/jsvine/pdfplumber/blob/stable/pdfplumber/table.py#L404\n",
    "            v_mid = (obj[\"top\"] + obj[\"bottom\"]) / 2\n",
    "            h_mid = (obj[\"x0\"] + obj[\"x1\"]) / 2\n",
    "            x0, top, x1, bottom = _bbox\n",
    "            return (h_mid >= x0) and (h_mid < x1) and (v_mid >= top) and (v_mid < bottom)\n",
    "        return not any(obj_in_bbox(__bbox) for __bbox in bboxes)\n",
    "    \n",
    "    return page.filter(not_within_bboxes)\n",
    "\n",
    "# helper function for pdfplumber    \n",
    "def remove_margins(page, dpi=72, size=0.7):\n",
    "    # strip 0.7 inches from top and bottom (page numbers, header text if any), A4 is 8.25 x 11.75\n",
    "    # syntax is page.crop((x0, top, x1, bottom))\n",
    "    w = float(page.width)/dpi\n",
    "    h = float(page.height)/dpi\n",
    "    return page.crop((0, (size)*dpi, w*dpi, (h-size)*dpi))\n",
    "\n",
    "\n",
    "# function: input file, output text of annex 1\n",
    "def read_smpc(filename, no_blanks=True, no_tables=True):\n",
    "    text = []\n",
    "    if filename.endswith('.pdf'):\n",
    "        with pdfplumber.open(filename) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page = remove_margins(page)\n",
    "                \n",
    "                if no_tables:\n",
    "                    page = remove_tables(page)\n",
    "                    \n",
    "                page_text = page.extract_text().split('\\n')\n",
    "                text += page_text\n",
    "    elif filename.endswith('.txt'):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            text = f.readlines()\n",
    "\n",
    "    annex_lines = [re.match('.*ANNEX\\s+I.*', line) is not None for line in text]\n",
    "    annex_index = [i for i,v in enumerate(annex_lines) if v]\n",
    "    \n",
    "    text = text[annex_index[0]:annex_index[1]]\n",
    "    if no_blanks:\n",
    "        text = [line for line in text if not line.isspace()]\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# function: input text, output list of section headers and content\n",
    "def get_smpc_sections(text):\n",
    "    idx, headers, sections = [], [], []\n",
    "    for i,line in enumerate(text):\n",
    "        if re.match('^[0-9]+\\.[0-9]*\\s+.*[A-Z].*', line):\n",
    "            idx += [i]\n",
    "            headers += [line.strip()]\n",
    "    \n",
    "    # in headers, must increment or restart, and not end in punctuation\n",
    "    idx_valid, headers_valid = [idx[0]], [headers[0]]\n",
    "    for n in range(1,len(headers)):\n",
    "        prev = float(headers[n-1].split()[0])\n",
    "        curr = float(headers[n].split()[0])\n",
    "        lastchar = headers[n].strip()[-1].lower()\n",
    "        valid = (prev < curr <= prev+1) or (curr==1)\n",
    "        valid = valid and (lastchar in 'qwertyuiopasdfghjklzxcvbnm()')\n",
    "        if valid:\n",
    "            idx_valid.append(idx[n])\n",
    "            headers_valid.append(headers[n])\n",
    "    idx, headers = idx_valid, headers_valid\n",
    "    \n",
    "    for n,h in enumerate(headers):\n",
    "        if (n+1)<len(headers):\n",
    "            contents = text[idx[n]+1:idx[n+1]]\n",
    "        else:\n",
    "            contents = text[idx[n]+1:]\n",
    "        sections += ['\\n'.join(contents)]\n",
    "    \n",
    "    return headers, sections\n",
    "\n",
    "\n",
    "def split_entries(headers, sections):\n",
    "    num_entries = sum([h.startswith('1.') for h in headers])\n",
    "    entries = [[] for _ in range(num_entries)]\n",
    "    entry = -1\n",
    "    for h,s in zip(headers, sections):\n",
    "        if h.startswith('1. '):\n",
    "            entry += 1\n",
    "        entries[entry] += [[h,s]]\n",
    "    return entries        \n",
    "    \n",
    "\n",
    "# function: save to file (by drug)\n",
    "def write_smpc_by_drug(entries, filename):\n",
    "    entries = [e for e in entries if len(e)]\n",
    "    for i,entry in enumerate(entries):\n",
    "        name = filename.split('/')[-1][:-4]\n",
    "        if len(entries)>1:\n",
    "            name = name + '_' + str(i+1)\n",
    "        with open('./output/'+name+'.txt', 'w', encoding='utf-8') as f:\n",
    "            for h,s in entry:\n",
    "                f.write(h+'\\n\\n')\n",
    "                f.write(s+'\\n\\n')\n",
    "\n",
    "# original workflow: run these four in series, no longer using fourth one\n",
    "\n",
    "\n",
    "#%% alternate method CLI conversion pdftotext.exe https://www.xpdfreader.com/download.html\n",
    "\n",
    "from subprocess import run\n",
    "\n",
    "errors, old_files, new_files = [], [], []\n",
    "txt_dir = data_dir + 'txt/'\n",
    "# See: https://www.xpdfreader.com/download.html\n",
    "exe_path = '~/Downloads/xpdf-tools-mac-4.04/bin64/pdftotext'\n",
    "flags = ['-layout', '-nodiag', '-enc', 'UTF-8', '-nopgbrk', '-marginb', '-54']\n",
    "for src in tqdm(os.listdir(pdf_dir)):\n",
    "    src = pdf_dir + src\n",
    "    tgt = src.replace(pdf_dir, txt_dir).replace('.pdf', '.txt')\n",
    "    command = [exe_path, *flags, src, tgt]\n",
    "    if not os.path.exists(tgt):\n",
    "        output = run(command, capture_output=True, shell=True)\n",
    "        if len(output.stderr):\n",
    "            errors += [(src, output.stderr)]\n",
    "        else:\n",
    "            new_files += [src]\n",
    "    else:\n",
    "        old_files += [src]\n",
    "        \n",
    "print(f'Converted {len(new_files)} PDFs to TXT, ({len(old_files)+len(new_files)}/{len(os.listdir(pdf_dir))}) complete, {len(errors)} errors')\n",
    "if(len(errors) > 0):\n",
    "    print(errors[0])\n",
    "        \n",
    "\n",
    "\n",
    "#%% initial pass at finding all headers\n",
    "# warning, this is very slow for pdf input, \n",
    "# txt input is faster (and more accurate), but we can't strip tables\n",
    "\n",
    "if RUN_DIAGNOSTICS:\n",
    "    all_headers = []\n",
    "    errors = []\n",
    "    input_dir = txt_dir\n",
    "    files = os.listdir(input_dir)\n",
    "    for file in tqdm(files):\n",
    "        try:\n",
    "            text = read_smpc(input_dir+file)\n",
    "            headers, sections = get_smpc_sections(text)\n",
    "            all_headers += headers\n",
    "        except:\n",
    "            errors += [file]\n",
    "    \n",
    "    all_headers = [re.sub('\\s+', ' ', h).strip() for h in all_headers]\n",
    "    all_headers = [h for h in all_headers if h[-1].lower() in 'qwertyuiopasdfghjklzxcvbnm()']\n",
    "    header_num = [float(h.split()[0]) for h in all_headers]\n",
    "    header_txt = [' '.join(h.split()[1:]).title() for h in all_headers]\n",
    "    counter = Counter(header_txt)\n",
    "    # 100 cutoff for header counts determined manually, if you inspect this dict\n",
    "    # you'll see obvious good ones with 1000+ and obvious bad ones with 1-10\n",
    "    centers = [h for h in counter if counter[h]>100]\n",
    "    centers.sort()\n",
    "    print(f'Found {len(centers)} common headers: \\n{centers}')\n",
    "\n",
    "\n",
    "#%% manually specify headers, use edit distance to do clustering\n",
    " \n",
    "centers = [\n",
    "    'Clinical Particulars',\n",
    "    'Contraindications',\n",
    "    'Date Of First Authorisation/Renewal Of The Authorisation',\n",
    "    'Date Of Revision Of The Text',\n",
    "    'Effects On Ability To Drive And Use Machines',\n",
    "    'Fertility, Pregnancy And Lactation',\n",
    "    'Incompatibilities',\n",
    "    'Interaction With Other Medicinal Products And Other Forms Of Interaction',\n",
    "    'List Of Excipients',\n",
    "    'Marketing Authorisation Holder',\n",
    "    'Marketing Authorisation Number',\n",
    "    'Name Of The Medicinal Product',\n",
    "    'Nature And Contents Of Container',\n",
    "    'Overdose',\n",
    "    'Pharmaceutical Form',\n",
    "    'Pharmaceutical Particulars',\n",
    "    'Pharmacodynamic Properties',\n",
    "    'Pharmacokinetic Properties',\n",
    "    'Pharmacological Properties',\n",
    "    'Posology And Method Of Administration',\n",
    "    'Preclinical Safety Data',\n",
    "    'Pregnancy And Lactation',\n",
    "    'Qualitative And Quantitative Composition',\n",
    "    'Shelf Life',\n",
    "    'Special Precautions For Disposal',\n",
    "    'Special Precautions For Disposal And Other Handling',\n",
    "    'Special Precautions For Storage',\n",
    "    'Special Warnings And Precautions For Use',\n",
    "    'Therapeutic Indications',\n",
    "    'Undesirable Effects'\n",
    "    ]\n",
    "# note: maybe we should manually merge these pairs:\n",
    "#   FERTILITY, PREGNANCY AND LACTATION\n",
    "#   PREGNANCY AND LACTATION\n",
    "#   SPECIAL PRECAUTIONS FOR DISPOSAL AND OTHER HANDLING\n",
    "#   SPECIAL PRECAUTIONS FOR DISPOSAL\n",
    "# but not doing so lets the similarity computation do its thing\n",
    "\n",
    "# improved initial text parsing step so this clustering problem wasn't so messy\n",
    "def get_fixed_header(text, centers=centers):\n",
    "    # return center with the lowest edit distance, \n",
    "    #   or placeholder (last entry) if no there's good match\n",
    "    dists = [levdistance(text.lower(),c.lower()) for c in centers]\n",
    "    #ix = np.argmin(dists)\n",
    "    ix = dists.index(min(dists))\n",
    "    if dists[ix] > 0.6*len(text):\n",
    "        return None\n",
    "    else:\n",
    "        return centers[ix]\n",
    "\n",
    "\n",
    "#%% use all the above to parse pdfs into sections\n",
    "\n",
    "input_dir = txt_dir\n",
    "files = os.listdir(input_dir)\n",
    "data = pd.read_excel('Medicines_output_european_public_assessment_reports.xlsx', skiprows=8, header=0)\n",
    "\n",
    "errors = []\n",
    "records = {}\n",
    "for file in tqdm(files):\n",
    "    try:\n",
    "        info = {}\n",
    "        product_code = file.split('.')[0]\n",
    "        row = data[data['Product number']==re.sub('_','/', file.split('.')[0])]\n",
    "        info['metadata'] = row.iloc[0].apply(str).to_dict()\n",
    "        \n",
    "        label_text = {} # next level = product page w/ metadata\n",
    "        text = read_smpc(input_dir+file)\n",
    "        headers, sections = get_smpc_sections(text)\n",
    "    \n",
    "        for h,s in zip(headers,sections):\n",
    "            header = get_fixed_header(h)\n",
    "            if (header is not None) and (len(s)>0):\n",
    "                if header not in label_text.keys():\n",
    "                    label_text[header] = [s]\n",
    "                else:\n",
    "                    label_text[header].append(s)\n",
    "        \n",
    "        info['Label Text'] = label_text\n",
    "        \n",
    "        records[row['Product number'].iloc[0]] = info\n",
    "\n",
    "    except:\n",
    "        errors += [file]\n",
    "\n",
    "print(f'Encountered problems reading {len(errors)} files')\n",
    "with open('data/output/human-rx-drug-ema.json', 'w') as f:\n",
    "    json.dump(records, f, indent=4)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada0d58-8f74-41b4-9b03-cf0db7860ded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
